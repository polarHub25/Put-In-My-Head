# 가상화(Virtualization)
## 가상화란 무엇이며, 왜 운영 체제에서 중요한가요?
> 가상화
- 물리적인 자원을 논리적으로 분리하거나 통합하여, 가상의 환경에서 효율적으로 사용할 수 있게 하는 기술.
  
> 운영체제에서 중요한 이유 
- 물리적 서버 하나에 여러 운영 체제를 실행할 수 있어서 하드웨어 자원의 사용률을 최적화 할 수 있음
- 가상화를 통해 물리적 하드웨어를 추가하지 않고도 새로운 애플리케이션을 배포하거나 테스트 등 서비스 확장에 유리
- 다양한 애플리케이션 실행 시, 가상화를 통해 각 애플리케이션을 독립된 상황에서 실행할 수 있어서 애플리케이션 간의 격리성 강화

> 가상화와 컨테이너 기술의 차이점은?
- 가상화는 물리적 하드웨어를 추상화하여 여러 개의 독립적인 가상 머신을 생성하는 기술. 각 가상머신은 별도의 운영체제를 실행 가능. 컨테이너처럼 커널을 공유하지않고, 하이퍼바이저 위에 구축된 완전한 가상의 컴퓨터.
- 컨테이너는 가상화와 비슷하게 격리된 환경을 제공하지만, 운영 체제의 커널을 공유. 

## 운영 체제가 CPU, 메모리, 디스크를 가상화한 것은 무엇인가요?
> CPU 가상화
- 물리적인 CPU 자원을 추상화하여, 여러 가상 머신이 물리적 CPU를 공유할 수 있게 만드는 기술
- 하이퍼바이저에 의해 관리되며, 가상 머신이 독립된 CPU를 사용하는 것처럼 동작하도록 함
  + 하이퍼바이저 : 가상 머신 간의 CPU 자원 사용을 관리하고 스케줄링하며 물리적 CPU에 대한 접근을 중재

> 메모리 가상화 ( 가상메모리 )
- 프로세스가 실제 물리적 메모리(RAM)보다 큰 메모리 공간을 사용하는 것처럼 보이게 하는 메모리 관리 기법
- 주요 키워드
  + 가상 주소 : 각 프로세스가 사용하는 메모리 주소 공간
  + 물리적 주소 : 실제 물리적 메모리에서의 데이터 저장 위치, 운영체제가 가상 주소를 물리적 주소로 변환하여 메모리에 접근
  + 페이징 : 가상 주소 공간과 물리적 메모리 공간을 고정된 크기의 블록으로 나누어 관리하는 방법
  + 페이지 : 가상 메모리의 블록 단위
  + 페이지 프레임 : 물리적 메모리의 블록 단위. 페이지와 동일한 크기로 나누어지며 운영체제는 각 페이지를 페이지 프레임에 매핑
  + 페이지 테이블 : 페이지와 페이지 프레임의 매핑 정보를 저장하는 데이터 구조. 프로세스마다 페이지 테이블은 따로 존재
  + 주소 변환 : OS는 MMU(Memory Management Unit)라는 하드웨어 장치를 통해 가상 주소를 물리적 주소로 변환
  + 프로세스가 특정 가상 주소에 접근하면 MMU는 페이지 테이블을 참조하여 해당 가상 주소에 매핑된 물리적 주소를 찾고, 접근함
  + 페이지 폴트(Page Fault) : 프로세스가 물리적 메모리에 존재하지 않는 페이지를 접근하려고 할때 발생하는 것
  + 운영체제는 페이지 폴트가 발생하면 해당 페이지를 디스크의 스왑(swap) 영역에서 물리적 메모리에 로드
  + 스왑(Swap) 영역 : OS는 물리적 메모리가 부족할때, 사용하지 않는 페이지를 디스크의 스왑영역에 임시로 저장하고 필요할때 다시 물리적 메모리로 호출
- 각 프로세스에게 독립된 메모리 공간을 제공하고 메모리 보호와 자원 관리를 최적화
- 페이지 폴트가 자주 발생하면 디스크I/O가 증가하게 되어 성능 저하 발생 (스래싱, Thrashing). 복잡한 메모리 관리

> 디스크 가상화 ( File system )
- 블록 단위로 물리적 디스크를 나누고, 블록들을 논리적으로 구성하여 파일과 디렉토리를 저장할 수 있는 구조 제공 

## 운영체제에서 커널이란 무엇이며 시스템 콜에 대해서 설명해주세요
> 커널이란? 
- 운영체제의 핵심으로 컴퓨터 자원(CPU, 메모리, 파일, 네트워크 입출력 장치 등)들을 관리하는 역할을 수행 

> 시스템 콜
- 응용 프로그램이 커널의 기능을 요청할 때 사용하는 인터페이스 (사용자가 작성한 프로그램은 커널에 직접 접근이 불가능) 
- 프로세스 제어(프로세스 생성, 종료, 메모리 할당 등 ), 파일 조작(파일 생성, 삭제, 읽기, 쓰기 등), 장치 관리(장치 요청 및 해제, 읽기, 쓰기 등), 정보 유지(시간 확인, 프로세스, 파일, 디바이스 속성 가져오기 등), 통신, 보안 

> 응용 프로그램 동작 예시
- 사용자가 "test.txt" 파일 클릭 -> 메모장 프로그램은 커널의 도움을 받기 위해 시스템 콜(open()) 호출 -> 커널은 받은 요청을 보고 파일 시스템을 통해 "test.txt" 파일이 실제 디스크에 있는지 확인하고, 해당 파일에 접근할 수 있는 파일 디스크립터 반환 -> 이후 메모장 프로그램은 파일의 내용을 읽기위해 시스템 콜(read()) 호출 -> 커널은 디스크에서 파일 내용 읽어서 메모장 프로그램에 반환 
- 즉, 커널은 하드웨어 자원 관리, 사용자와 응용 프로그램이 요청한느 다양한 작업 처리
- 시스템 콜은 응용 프로그램이 커널에 자원을 요청할때 사용하는 인터페이스 

https://goodmilktea.tistory.com/23

## 인터럽트란 무엇이며 인터럽트와 시스템 콜의 차이점에 대해서 설명해주세요.
> 인터럽트(Interrupt)
- CPU가 현재 수행 중인 작업을 일시 중단하고, 외부 하드웨어나 소프트웨어 예외에 대응할 수 있게 하는 메커니즘 

> 인터럽트와 시스템 콜의 차이점
- 인터럽트는 주로 비동기적 이벤트로 CPU가 즉각적으로 현재 작업을 중단하고 처리할 때 사용됨. 키보드 입력, 네트워크 수신 등 예기치 않은 상황에 대응할 수 있도록 시스템의 반응성 향상
- 시스템 콜은 사용자 프로그램이 커널의 기능을 사용하기 위해 명시적으로 요청하는 주로 동기적 이벤트. 프로그램이 파일을 열거나 메모리를 할당할 때 시스템 콜을 호출하여 커널에게 작업을 요청. 

## 멀티프로세싱 시스템에서 스케줄링이 중요한 이유는 무엇인가?
> 멀티 프로세싱이란?
- 다수의 CPU를 사용하여 동시에 여러 작업을 병렬로 처리할 수 있는 시스템
- 성능 향상, 확장성 <-> 복잡한 동기화, 비용 증가

> 스케줄링이 중요한 이유
- 스케줄링 : 여러 개의 CPU가 작업을 효율적으로 수행할 수 있도록 프로세스나 스레드에 CPU 시간을 할당하는 작업
- 여러 CPU에 작업을 효율적으로 분배하여 시스템 성능을 극대화 하기 위함. 스케줄링을 통해 CPU 자원을 최대한 활용하고, 응답 시간과 처리 시간을 최적화하며 자원을 공정하게 배분해야함. 이를 통해 시스템의 처리량을 높이고 프로세스간의 동시성을 관리. 

https://inpa.tistory.com/entry/%F0%9F%91%A9%E2%80%8D%F0%9F%92%BB-multi-programming-tasking-processing

## 컨텍스트 스위칭이란 무엇이며, 컨텍스트 스위칭에 따라 오버헤드가 발생하는 이유가 무엇인가요?
- CPU가 현재 실행 중인 프로세스의 상태를 저장하고, 다른 프로세스의 상태 및 정보를 읽어 실행을 전환하는 과정 

> context switching 과정
1. 현재 프로세스 상태 저장
   - 레지스터 저장 : CPU는 현재 실행중인 프로세스의 레지스터값(프로그램 카운터, 스택 포인터 등)을 해당 프로세스의 PCB에 저장
2. 스케줄러에 의해 새로운 프로세스 선택
   - 스케줄러가 실행되어 다음에 CPU에 사용할 프로세스를 선택. 스케줄러는 우선순위, 대기 시간 등을 기준으로 다음 프로세스 결정
3. 새로운 프로세스의 상태 복원
  - 레지스터 복원 : 스케줄러에 의해 선택된 프로세스의 PCB에 저장되어 있던 레지스터 정보들을 CPU 레지스터에 다시 로드
  - 프로그램 카운터 복원 : 이전에 해당 프로세스가 중단되었던 위치에서부터 명령어 실행을 재개
  - 메모리 매핑 : 프로세스가 사용하는 가상 메모리 주소를 실제 메모리에 매핑하는 페이지 테이블 등의 정보 복원
4. 복원된 상태 정보를 바탕으로 새로운 프로세스 실행 

> context switching에서 오버헤드가 발생하는 이유는?
- 위에 처럼 context switching 과정 중에 CPU가 프로세스간에 전환 수행시에 필요한 추가 작업들로 인해 오버헤드가 발생할 수 있음
1. 현재 프로세스 저장시에도 레지스터 값을 PCB로 저장하는 과정에서 CPU는 실제 작업을 수행하지않고, 저장하는 작업을 하고있으므로 오버헤드 발생
2. 스케줄러 알고리즘이 실행되어 선택하는 과정에도 오버헤드 발생 가능 
3. 캐시미스와 같은 메모리와 관련된 작업 전환시에 오버헤드 발생 가능
   - 캐시미스 : 프로세스 전환 시 이전 프로세스의 데이터가 캐시에 저장되어 있다가 새로운 프로세스가 로드되면 캐시의 기존 데이터는 무효화 되며 이때 캐시 미스가 발생. 이 경우 CPU는 메모리에서 새로운 데이터를 다시 로드해야하므로 추가적인 리소스 소비

## CPU 스케줄링 알고리즘의 종류와 특징은?(라운드 로빈, 우선순위 스케줄링, 멀티 레벨 큐)
> 라운드 로빈 (Round Robin)
- 선점형 스케줄링의 하나로, 프로세스들 사이에 우선순위를 두지않고, 순서대로 시간단위로 CPU를 할당하는 방식의 스케줄링 알고리즘
- 모든 프로세스는 동일한 시간 단위를 할당받으며, 할당 시간이 종료되면 대기열의 맨 뒤로 이동하고 다음 프로세스 실행
- 구현이 간단하지만, 타임 슬라이스 설정이 적절하지 못한 경우 성능이 저하될수 있고, 컨택스트 스위칭 비용이 증가할 수 있음(짧은 타임 슬라이스의 경우 프로세스간의 전환이 자주 발생시) 

> 우선순위 스케줄링 (Priority Scheduling)
- 각 프로세스의 우선순위 값을 할당하고, 우선순위가 높은 프로세스부터 CPU 할당하는 방식
- 우선순위 스케줄링 종류
  + 선점형 우선순위 스케줄링 : 현재 실행 중인 프로세스보다 우선순위가 높은 프로세스가 도착 시 CPU를 강제로 빼앗아 새로운 프로세스에 할당
  + 비선점형 우선순위 스케줄링 : 현재 실행 중인 프로세스가 CPU를 반환할 때까지 대기한 후, 우선순위가 높은 프로세스가 실행
- 응답시간이 중요한 시스템에 유리
- 우선순위가 낮은 프로세스는 우선순위가 높은 프로세스 때문에 오랫동안 CPU를 할당받지 못하는 Starvation 상태가 발생할 수 있음

> 멀티 레벨 큐 스케줄링 (Multi-level Queue Scheduling)
- 프로세스들을 여러 개의 큐로 분류하여, 각각의 큐에 다른 스케줄링 알고리즘을 적용하는 방식
- 대화형 프로세스는 높은 우선순위 큐, 배치 작업은 낮은 우선순위 큐에 넣는 식으로 관리 가능
- 각 큐는 독립적으로 다른 스케줄링 알고리즘을 사용할 수 있어 시스템 요구에 맞게 스케줄링 방식 조정이 가능
- 설정이 복잡하고, 각 큐의 우선순위와 작업 분배를 잘못하면 Starvation 상태가 발생

https://jwprogramming.tistory.com/17

# 프로세스와 쓰레드
## 프로세스와 스레드의 차이점은 무엇인가?
> 프로세스
- 실행 중인 프로그램의 인스터스로, 운영체제로부터 독립적인 메모리 공간을 할당받아 실행(컴퓨터에서 작업중인 프로그램)
- 프로세스 구조 : 코드, 데이터, heap, stack으로 구성된 자체 메모리 공간이며, 다른 프로세스와 메모리를 공유하지않음
- 프로세스는 PCB라는 데이터 구조를 통해 관리 
> 스레드
- 프로세스 내에서 실행되는 작업의 흐름으로, 프로세스의 자원을 공유하며 실행되는 작은 실행 단위
- 하나의 프로세스는 여러개의 스레드를 가질 수 있고, 해당 스레드들은 코드, 데이터, heap은 공유하지만 stack은 개별적으로 관리

https://inpa.tistory.com/entry/%F0%9F%91%A9%E2%80%8D%F0%9F%92%BB-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-%E2%9A%94%EF%B8%8F-%EC%93%B0%EB%A0%88%EB%93%9C-%EC%B0%A8%EC%9D%B4

## PCB(Process Control Block)란 무엇이며, 어떤 정보를 포함하나요?
- PCB 란 , 운영체제에서 프로세스를 관리하기 위해 해당 프로세스의 상태 정보를 담고있는 자료구조 ( 프로세스 스케줄링을 위한 프로세스 정보에 대한 임시 저장소 ) 
> PCB에 저장되는 정보
- 포인터 (Pointer): 프로세스의 현재 위치를 저장하는 포인터 정보
- 프로세스 상태 (Process state) : 프로세스의 각 상태를 저장 (생성(New), 준비(Ready), 실행(Running), 대기(Waiting), 종료(Terminated))
- 프로세스 식별자 (Process ID, PID) : 프로세스 고유하게 식별할 수 있는 고유 번호
- 프로그램 카운터 (Program Counter) : 현재 실행 중인 명령어의 위치를 가리키는 주소
- 레지스터 (Register) : CPU 레지스터에 있는 정보
- 메모리 관리 정보 (Memory Limits) : 해당 프로세스의 주소 공간 
- 입출력 상태 정보 : 프로세스에 할당된 입출력장치 목록 등
- CPU 스케줄링 정보 : 우선순위, 최종 실행시간, CPU 점유시간 등

## 프로세스 주소 공간이란 무엇이며, 프로세스마다 고유한 주소 공간을 가지는 이유는?
> 프로세스 주소공간
- 프로세스가 실행되는 동안 접근할 수 있는 메모리 영역
- OS는 프로세스마다 독립적인 주소 공간을 할당

> 주소 공간 구조
- Code
  + 프로그램의 실행 코드(명령어)가 저장되는 영역
  + 실행파일(.exe) 등의 명령어가 로드되며, 읽기 전용으로 설정되고 해당 영역은 수정 불가능 
- Data
  + 초기화된 전역 변수, 정적 변수가 저장되는 영역
  + 프로그램이 실행되기 전 컴파일 시점에 초기화된 데이터들이 해당 영역에 위치 
- Heap
  + 동적으로 할당된 메모리가 저장되는 영역
  + 런타임 동안 크기가 변경될 수 있고, 메모리 할당 및 해제 작업에 따라 동적으로 변경될 수 있음
  + 프로세스가 직접 관리해야하며, 메모리 누수가 발생할 수 있음 
- Stack
  + 함수 호출 시 생성되는 지역변수, 함수 호출 정보(리턴주소, 매개변수 등)가 저정되는 영역
  + 함수가 호출될때마다 쌓이며, 종료되면 할당 공간 해제
 
> 고유한 주소 공간을 가지는 이유
- 프로세스간 메모리 보호 및 보안 : 각 프로세스는 독립된 주소 공간을 가지므로 다른 프로세스 메모리에 접근할 수 없음 -> 프로세스간 메모리 보호 가능하며, 하나의 프로세스에서 발생한 오류가 다른 프로세스에 영향을 미치는 것을 방지
- 프로그램 안정성 및 충돌 방지 : 각 프로세스는 자신의 메모리 영역만을 사용하므로 메모리 충돌이 발생하지 않음
- 여러 프로세스를 동시에 실행할 수 있음

## 10/27 
## 프로세스 상태 전이(Process State Transition)를 설명해보세요.
> 프로세스 상태
- create/new : 프로세스가 생성되는 단계 
- ready : 프로세스가 생성되어 메인메모리에 적재가 되고, 필요한 자원을 모두 얻은 상태
- running : 프로세스가 cpu를 점유하여 실행중인 상태
- terminated : 프로세스가 종료되는 상태
- asleep : 메인메모리에 적재는 되었지만 필요한 자원을 얻지 못한 상태
- suspended ready : 프로세스가 실행 준비는 완료되었지만, 메모리 부족으로 인해 보조기억장치로 이동된 상태. 메모리로 다시 로드되면 Ready 상태.
- suspended blocked : 프로세스가 특정 I/O 작업이나 이벤트를 기다리는 동안 메모리 부족으로 인해 보조기억장치로 이동된 상태. 해당 작업이 완료되면 Suspended Ready 상태로 전환

> 상태 전이 
- Dispatch : 프로세스 스케줄러에 의해 결정된 우선순위에 따라 프로세스가 CPU를 점유한 상태
- TIme out : 프로세스가 제한된 시간을 다 소비하여 CPU 점유를 빼앗기는 상태
- Block : 실행중이던 프로세스가 외부 요인에 의해서 자원을 빼앗기는 상태
- Wake up : 프로세스가 자원을 할당받은 상태
- Swap in : 프로세스가 메인메모리에 적재 되는 상태
- Swap out : 프로세스가 메인메모리에서 해제 되는 상태 

## 프로세스 간 통신(IPC: Interprocess Communication) 방법에는 무엇이 있나요? (파이프, 메시지 큐, 공유 메모리 등)
> 파이프 (Pipe)
- 프로세스 간 데이터 스트림을 통해 단방향으로 통신할 수 있는 방법
- 부모-자식 프로세스 간 단방향(반이중) 통신에 자주 사용
- Unix/Linux 에서 pipe() 명령어 호출하여 생성
- 데이터를 쓰는 프로세스는 파이프가 찰 때까지 데이터를 보내고, 읽는 프로세스는 버퍼에서 데이터를 읽고, 읽는 쪽은 파이프에 데이터가 없으면 대기 상태로 들어감 
- 파이프의 버퍼는 일반적으로 작아서 프로세스 간에 빠르게 데이터를 교환할때 사용하는 것이 적합
- 예시) 쉘 명령어 ls | grep ".log"
- https://hwan-shell.tistory.com/324
  
> 메시지 큐 (Message Queues)
- 프로세스 간에 메시지를 교환할 수 있도록 커널에서 제공하는 데이터 구조
- 프로세스가 특정 큐에 메시지를 저장하고 다른 프로세스가 그 메시지를 꺼내어 사용하는 방식
- msgget() 메시지 큐 생성 -> msgsnd 메시지 추가 -> msgrcv 큐에서 메시지 읽기
- FIFO 방식
- 비동기 통신이 가능 -> 메시지를 보낸 프로세스는 메시지가 수신완료되길 기다리지 않고 다른 작업 수행 가능
- 메시지 관리 및 처리 로직이 복잡할 수 있음
- 예시) 백엔드에서 작업을 메시지 큐에 넣고, 여러 프로세스가 이를 꺼내 병렬로 작업을 처리
  
> 공유 메모리 (Shared Memory)
- 여러 프로세스가 동일한 메모리 영역을 직접 공유하여 데이터를 주고받는 방식
- shmget 공유 메모리 영역 생성 -> shmat 공유 메모리 공간을 프로세스의 메모리 공간에 연결 후 사용 -> 연결 후 프로세스는 공유 메모리의 시작 주소를 포인터로 받아서 사용
- 여러 프로세스가 동일한 메모리 공간에 접근하므로, 동기화를 이용해 데이터의 일관성 유지해야함
- 메모리 접근 속도가 빠르며, 대량의 데이터를 효율적으로 전송 가능
- 동기화를 별도로 구현해야 하므로 동기화 문제 발생 시 복잡성 증가. 메모리 누수 발생 가능성 존재
- 예시) 실시간 데이터 처리, 대용량 데이터 처리

## 멀티스레드(Multithreading)의 장점과 단점은 무엇인가요?
- 멀티 스레드는 하나의 프로세스 안에 여러개의 스레드가 있는 것
> 장점
- 멀티스레드는 스택을 제외한 동일한 메모리 공간을 공유하므로 멀티프로세스보다 데이터 공유가 쉽고 빠르므로 비용 절약과 메모리 사용량 절약 가능
- 프로세스의 컨텍스트 스위칭보다 빠르며 오버헤드가 감소해 응답시간이 빠름 

> 단점
- 하나의 스레드에서 문제가 발생하면 다른 스레드들도 영향을 받아 프로그램이 종료될수 있음
- 여러 스레드가 동시에 공유 자원에 접근하여 동기화 문제 발생
- 뮤텍스, 세마포어같은 동기화 처리 방식이 있지만, 잘못 설계시 데드락, 경쟁상태같은 문제 발생 

## 스레드 동기화(Thread Synchronization)가 중요한 이유는 무엇인가요?
- 여러 스레드가 자원을 안전하게 사용하도록 보장하고, 프로세스의 실행 순서를 보장하기 위해서 중요.

# 동시성
## 동시성 문제란 무엇인가?
- 여러 스레드나 프로세스가 동시에 같은 데이터를 읽고 쓰는 과정에서 데이터의 일관성이 깨지는 상황

> 경쟁 상태 (Race Condition) 
- 2개 이상의 스레드가 동시에 공유 자원에 접근하여 실행 순서에 따라 결과가 달라지는 상황
> 데드락 (Deadlock)
- 여러 프로세스가 서로 상대방이 점유한 자원을 기다리며 무한히 대기하는 상태 
> 기아 상태 (Starvation)
- 스레드 또는 프로세스가 필요한 자원을 지속적으로 할당받지 못해 실행되지 못하는 상태 ( 주로 우선순위가 낮은 작업 )
> 데이터 불일치
- 데이터의 일관성이 깨져 서로 모순되는 데이터가 존재하는 상태

## 교착 상태(Deadlock)란 무엇인가요? 교착 상태의 해결 방법을 설명해보세요.
- 여러 프로세스나 스레드가 서로 상대방이 점유한 자원을 기다리며 무한히 대기하는 상태

> 교착 상태 발생 조건 ( 4가지 모두 성립해야 데드락 발생 ) 
- 상호 배제(Mutual Exclusion)
  + 자원은 한번에 한 프로세스만 사용 가능
- 점유 대기(Hold and Wait)
  + 최소한 하나의 자원을 점유하고 있으면서 다른 프로세스에 할당되어 사용하고 있는 자원을 추가로 점유하기 위해 대기하는 프로세스가 존재해야함 
- 비선점(Non-Preemption)
  + 다른 프로세스에 할당된 자원은 사용이 끝날 때까지 강제로 점유할 수 없음 
- 순환 대기(Circular Wait)
  + 프로세스의 집합에서 순환 형태로 자원을 대기하고 있어야함
  + 프로세스 A 자원 X:점유 /자원 Y:요청
  + 프로세스 B 자원 Y:점유 /자원 X:요청

> 교착 상태 해결 방법
- 교착 상태 예방 (Deadlock Prevention)
  + 교착 상태 4가지 조건 중 하나를 제거하여 교착 상태 방지
  + 상호 배제 제거 : 자원을 여러 프로세스가 동시에 접근할 수 있도록 허용하는 방법
  + 점유 대기 제거 : 프로세스가 자원을 요청할 때 현재 점유 중인 자원을 모두 해제한 후 다시 요청하도록 하는 방법 
  + 비선점 제거 : 자원을 점유한 프로세스가 추가 자원을 요청할 때, 요청한 자원을 즉시 할당할 수 없는 경우 기존에 점유한 자원을 해제하고 다른 프로세스가 사용하는 방법
  + 순환 대기 제거 : 자원에 고유한 번호를 할당하고, 자원을 요청할 때 순서대로만 요청하게 하는 방법
- 교착 상태 회피 (Deadlock Avoidance)
  + 교착 상태가 발생할 수 있는 상황을 실시간 분석하여 안전한 상태에서만 자원을 할당
  + 은행원 알고리즘
  + 프로세스가 자원을 요구할 때, 시스템은 자원을 할당한 후에도 안정 상태로 남아있게 되는지 사전에 검사하여 교착 상태 회피
  + 안정 상태면 자원할당 , 아닌 경우 다른 프로세스들이 자원해지까지 대기
- 교착 상태 탐지 (Deadlock Detection) 및 회복 (Recovery)
  + 교착 상태가 발생할 수 있도록 허용한 후, 교착 상태를 주기적으로 검사하고, 발생 시 회복하는 방법
  + 탐지 : 자원의 상태와 프로세스의 대기 그래프를 주기적으로 검사하여, 순환 대기가 있는지 확인. 그래프에서 사이클이 발견되면 교착 상태가 발생
  + 회복
    + 프로세스 종료 : 교착 상태에 있는 프로세스들 중 일부를 종료하여 자원을 해제
    + 자원 선점: 교착 상태에 있는 프로세스 중 일부의 자원을 강제로 회수하여 다른 프로세스가 사용할 수 있도록 함
- 교착 상태 무시 (Ignore Deadlock)
  + 데드락의 발생 확률이 비교적 낮은 경우 별다른 조치 없이 무시하는 방법


## 임계 구역(Critical Section)이란 무엇이며, 이를 해결하는 방법은 무엇인가요?
> 임계 구역(Critical Section)
- 공유 데이터의 일관성을 보장하기 위해 하나의 프로세스/스레드만 진입해서 실행 가능한 영역역

> 임계 구역에서 발생할 수 있는 문제
- 경쟁 상태, 데드락, 기아상태 등

> 해결 방법
- 뮤텍스(Mutex), 세마포어(Semaphore), 모니터(Monitor)

> 뮤텍스(Mutex)
- 임계 구역에 한번에 하나의 스레드만 접근할 수 있도록 하는 Lock
- 하나의 스레드가 임계 구역에 진입할 때 락을 획득하고, 작업이 끝나면 락을 해제함으로써 다른 스레드가 접근할 수 있게 하는 방식
- 간단한 구현, 데이터 일관성 보장 (경쟁 상태 방지)
- 락 해제, 획득 순서로 인해 데드락 발생 가능성 있음 
> 세마포어(Semaphore)
- 카운터 값을 사용해 여러 스레드가 동시에 접근할 수 있는 자원의 수를 제한하는 동기화 도구
- 다수의 스레드가 자원에 동시에 접근할수 있음
- 데드락 가능성 존재
- 세마포어의 값을 수동으로 관리해야 하므로 복잡한 시스템에서는 오류 발생할 수 있음 
> 모니터(Monitor)
- 공유 자원을 내부적으로 숨기고 공유 자원에 접근하기 위한 인터페이스만 제공함으로써 자원을 보호하고 프로세스 간에 동기화 적용
- 객체 내부에 임계 구역에 대한 락과 조건변수를 포함하여 스레드들이 객체의 특정 메서드나 코드 블록에 한번에 하나씩만 접근할 수 있도록 관리하는 방식
- 자바에 synchronized
- 프로그래머가 직접 락을 관리하지 않고, 프로그래밍 언어에서의 모니터를 통해 동기화 구현 가능
- 복잡한 동작 제어

## 상호 배제(Mutual Exclusion)란 무엇인가요?
- 여러 프로세스나 스레드가 동시에 공유 자원에 접근하지 못하도록 하는 원칙
- 데이터의 일관성과 프로그램의 안정성을 보장하며 경쟁 상태 방지
- 뮤텍스, 세마포어, 모니터와 같은 기법이 상호 배제를 구현하기 위한 방식

- https://yoongrammer.tistory.com/61

## 뮤텍스(Mutex)와 세마포어(Semaphore)의 차이점은 무엇인가요?
- 사용목적과 동작 방식에 차이가 존재
- 뮤텍스는 단일 스레드의 접근만 허용. 세마포어는 카운터 값에 따라 여러 스레드의 동시 접근을 제어
- 뮤텍스는 락을 획득한 스레드만이 해제할 수 있지만, 세마포어는 자원을 점유하지 않은 스레드도 카운터를 증가 시킬수 있음
- 뮤텍스는 단일 자원의 접근 보호에 적합하며, 세마포어는 여러 자원의 접근 수를 제한하고 관리하는 용도로 적합 

## 스핀락(Spinlock)과 같은 동기화 기법은 언제 사용되나요?
- 스핀락 : 락을 획득할 때까지 계속해서 락의 상태를 확인하는 방식
- CPU를 사용해 반복적으로 락을 확인

> 사용 예시
- 멀티코어 시스템에서 락 대기 시간이 짧은 경우
  + 락이 해제될때까지의 시간이 짧은 경우에는 스핀락이 락을 기다리며 반복 확인하는 것이 스레드의 상태를 전환하는 것보다 더 빠르고 효율적일수 있음
- 오버헤드를 줄이고 싶은 경우
  + 일반적인 락의 경우 스레드가 락을 얻지 못하면 블로킹 상태로 전환 -> 락이 해제되면 다시 깨어나서 작업 재개 식의 과정에서 컨텍스트 스위칭이 발생하는데 이는 오버헤드를 발생. 이 경우 락이 짧은 시간내에 해제가 될 거같은 경우에는 스핀락으로 오버헤드를 줄일 수 있음
    
## Readers-Writers, 생산자-소비자 문제와 식사하는 철학자 문제에 대해서 설명해주세요.
- 동시성 제어 관련해서 데이터 일관성, 효율성, 자원 사용의 공정성 관리에 중점 

> Readers-Writers
- Reader : 공유 버퍼의 내용을 읽기만 하는 프로세스
- Writer : 공유 버퍼에 내용을 갱신하는 프로세스 
- 하나의 writer가 공유 자원에 접근하고 있을때 다른 reader, writer 프로세스가 동시에 접근할 경우 혼란이 발생 
- Readers Priority
  + 여러 reader는 동시에 자원에 접근할 수 있지만, writer는 reader의 자원 사용이 완료될때까지 대기
  + reader가 많은 경우 시스템 개선
  + writer가 오래 대기할 경우 writer starvation 문제가 발생할 수 있음
- Writers Priority
  + writer가 자원에 접근할 우선권을 가지고, writer가 대기중인 경우 모든 reader는 대기
  + writer의 대기 시간이 짧아지고, 빠르게 자원에 접근 가능
  + reader가 오랜 시간 대기하게 되는 reader starvation 문제 발생할 수 있음 
- 사용 예시 : db 에서의 드랜잭션 , 파일 시스템 , 캐시 시스템 

> 생산자-소비자 문제
- 생산자 : 데이터 공급 ( Inqueue )
- 소비자 : 데이터 처리 ( Dequeue )
- 하나의 공유 버퍼를 두고 데이터를 생산하는 생산자와 데이터를 소비하는 소비자 사이에서 발생하는 동기화 문제 
- 세마포어를 활용한 해결방법
  + full 세마포어와 empty 세마포어를 사용하여 버퍼의 상태를 확인하고, 이진 세마포어를 통해 생산자와 소비자가 동시에 버퍼를 수정하지않도록 함
  + full 세마포어 : 현재 버퍼에 있는 데이터의 수. 소비자가 버퍼에서 데이터를 꺼낼때 감소
  + empty 세마포어 : 버퍼에 남아있는 빈 공간의 수. 생산자가 데이터를 추가할 때 감소
  + 이진세마포어 : 생산자,소비자가 동시에 버퍼를 수정하지 못하도록 상호 배제 제공
  + 구현이 비교적 간단하며, 각 세마포어의 역할이 명확하므로 관리가 용이
  + 세마포어 수를 잘못 제어할 경우 데드락이나 경쟁 상태가 발생할 수 있음 
- 모니터를 활용한 해결방법
  + 상호 배제와 조건 변수를 활용하여 버퍼 상태에 따라 생산자, 소비자 접근을 조율.
  + 생산자, 소비자가 각각 대기 조건을 만족할 때까지 락을 걸고, 조건이 충족되면 신호를 통해 대기중인 프로세스 깨움
  + 세마포어보다 높은 수준의 추상화로 동기화 제어 가능
  + 일부 언어에서는 기능이 없거나 별도의 라이브러리가 필요 
- 예시 : 비동기 메시지 시스템 -> rabbitmq 같은 메시지 큐 시스템에서 프로듀서가 메시지를 큐에 추가하면, 컨슈머가 이를 소비하여 다른 서비스에 전달하거나 처리
  
> 식사하는 철학자 문제
- 다수의 프로세스가 공유 자원을 사용할 때 발생하는 동기화 문제의 대표적 사례.
- 교착 상태와 기아 상태 방지를 위해 고안된 문제 
- 웨이터 방식
  + 웨이터 역할의 중앙 관리자가 철학자가 자원을 요청할때 2개의 젓가락이 모두 사용 가능할 때만 접근을 허용하는 방식
  + 교착 상태를 방지할수 있고, 모든 철학자가 공평하게 자원을 사용할 수 있음
  + 중앙 관리자 프로세스가 추가되어 오버헤드가 발생할 수 있음 
- 자원 계층 방식
  + 자원의 순서를 지정하여 모든 철학자가 동일한 순서로 자원을 잡지 않도록 하여, 모든 철학자가 동시에 대기하는 순환 대기조건을 방지하는 방식
  + 구현이 비교적 간단하지만, 자원 사용 효율이 다소 떨어질 수 있음

## 조건 변수(condition variable)란 무엇인가요?
- 특정 조건을 만족하기를 기다리는 변수
- 멀티스레드에서 스레드 간의 통신 및 동기화를 위해 설계된 변수
- 조건 변수의 메서드
  + wait() : 현재 스레드는 뮤텍스를 해제하고 조건 변수가 신호를 받을 때까지 대기
  + notify(), signal() : 대기 중인 스레드 중 하나를 깨움
  + notifyAll() : 대기 중인 모든 스레드를 깨움
- 예시 ) 생산자-소비자 문제에서 생산자는 버퍼에 빈 공간이 있을때만 데이터를 추가하고 소비자는 버퍼에 데이터가 있을때만 데이터를 가져갈 수 있는데 이 조건을 충족시키기 위해 사용하는 변수가 조건변수 

# 메모리 관리 (Memory Management)

## 메모리 계층 구조에 대해서 설명해주세요.
- 프로세스 실행 시 필요한 데이터와 명령어를 CPU가 효율적으로 접근 할 수 있도록 메모리 계층을 나눠서 설계

> 레지스터
- CPU 내부에 위치한 고속 메모리
- CPU가 요청을 처리하는데 필요한 데이터를 저장하고 접근할 수 있는 임시 저장소
- 구성 요소
  + 프로그램 카운터 (PC) : 다음에 실행할 명령어의 주소 저장
  + 명령어 레지스터 (IR) : 현재 실행 중인 명령어 저장
  + 누산기 (Accumulator) : 연산 중간 결과를 저장
  + 상태 레지스터 (Flags) : 명령에 대한 상태 기록
- 동작 원리
  + 레지스터 간의 데이터 이동은 메모리간 이동 없이 CPU 내부에서 실행되므로 속도가 매우 빠름 

> 캐시메모리
- CPU와 RAM 사이에서 빈번하게 사용되는 데이터를 저장하는 메모리
- 자주 필요한 데이터를 저장해 CPU의 대기 시간을 줄이고, 빠르게 접근할 수 있도록 함
- 구성 요소 
  + L1 캐시 : 각 CPU 코어에 개별적으로 할당되며, 매우 빠르지만 용량 작음
  + L2 캐시 : CPU 코어에 연결되지만, L1보다 용량이 크고 속도는 더 느림
  + L3 캐시 : 여러 CPU 코어가 공유하며, 캐시중 용량이 가장 크지만 속도가 느림
- 동작 원리
  + Cache Hit, Cache Miss에 따라 작동
  + CPU가 데이터를 요청할 때 캐시를 먼저 찾고 데이터가 있으면 사용. 없으면 RAM에서 불러온 후 캐시에 저장한 뒤 사용

> 주기억장치 (Main Memory)
- 현재 CPU가 처리하고 있는 내용을 저장하는 기억장치
- 구성 요소
  + ROM( Read Only Memory ) : 비휘발성 메모리. 데이터를 저장한 후 반영구적으로 사용 가능.
  + RAM ( Random Access Memory ) : 휘발성 메모리. 응용 프로그램, 운영체제 등을 불러와서 CPU가 작업할 수 있도록 하는 기억장치 

> 보조기억장치 (Secondary Storage, HDD/SSD)
- 물리적인 디스크가 연결되어 있는 기억장치
- 주기억장치보다는 느리지만 전원을 종료해도 데이터를 영구적으로 보관하는 장치
- 구성요소
  + HDD ( Hard Disk Driver ) : 물리적인 디스크를 고속으로 회전시켜 데이터를 저장하는 장치
  + SSD (Solid State Driver ) : 물리적인 아닌 전기적으로 데이터를 저장하므로 HDD보다 빠름
  
> 가상 메모리 (Virtual Memory)
- 컴퓨터 시스템에서 실제 물리적 메모리(RAM)의 크기를 확장하기 위해 사용하는 메모리 관리 기법
- 구성요소
  + 페이지테이블, 페이지, 프레임으로 구성
  + 페이지 테이블을 통해 가상 주소와 물리 주소간의 매핑 관리
- 동작 원리
  + 페이징을 통해 필요할 때마다 하드디스크에서 데이터를 불러 RAM에 올림.
  + 필요한 데이터가 RAM에 없는 경우 하드 디스크의 영역에서 데이터를 읽어와 작업 진행
    
## 메모리에서 스택, 힙, 코드, 데이터 영역에 대해서 설명해주세요.
> 코드 영역
- 실행할 프로그램의 명령어와 함수가 저장되는 곳
- CPU는 코드 영역에 저장된 명령을 하나씩 가져가서 처리 
- 읽기 전용으로 설정되어 프로그램이 실행되는 동안 내용이 변경되지않음
- 프로그램이 시작하고 종료될때까지 메모리에 계속 남아있음

> 데이터 영역
- 프로그램의 전역 변수와 정적 변수가 저장되는 영역
- 프로그램 시작시 메모리가 할당되며 프로그램이 종료되면 메모리 해제
  
> 힙 영역
- 프로그램 실행 중 동적으로 메모리를 할당받아 사용하는 영역.
- 자바의 경우 가비지 컬렉션이 메모리 관리를 수행. 더이상 사용하지 않는 메모리는 회수 
- 동적 메모리 관리를 제대로 하지 못하면 메모리 누수 발생 
- FIFO 방식 / new를 통해 동적으로 할당된 배열, 객체 등이 저장
  
> 스택 영역
- 함수 호출 시 지역 변수와 매개변수 등을 저장하는 공간. 함수의 흐름 관리
- 메모리 할당과 해제가 빠르며 자동으로 관리되어 사용 간편
- 스택의 크기가 제한적이어서 많은 메모리를 할당하거나 재귀 호출이 발생하면 스택 오버플로우 발생 가능
- LIFO 방식

## 가상 메모리(Virtual Memory)란 무엇인가요?
- 컴퓨터 시스템에서 실제 물리적 메모리(RAM)의 크기를 확장하기 위해 사용하는 메모리 관리 기법
- 가상 주소 : 프로세스가 참조하는 주소. 프로그램이 메모리에 데이터를 저장할 때 사용하는 논리적인 주소
- 물리 주소 : 실제 메모리(RAM)에서 데이터를 저장하는 실제 주소
- CPU는 가상 주소를 생성 -> 메모리 관리 장치 MMU(Memory Management Unit)를 통해 가상 주소를 물리주소로 변환하여 데이터를 참조
- 장점 : 메모리 확장 , 프로세스간 메모리 격리 ( 각 프로세스가 독립적인 가상 주소 공간을 가지므로 메모리 보호 )
- 단점 : 페이지 폴트 발생 시 디스크에서 데이터를 로드하는 과정이 오래걸려 성능 저하 , 오버헤드 증가 


  + 페이징 : 가상 주소 공간과 물리적 메모리 공간을 고정된 크기의 블록으로 나누어 관리하는 방법
  + 페이지 : 가상 메모리의 블록 단위
  + 페이지 프레임 : 물리적 메모리의 블록 단위. 페이지와 동일한 크기로 나누어지며 운영체제는 각 페이지를 페이지 프레임에 매핑
  + 페이지 테이블 : 페이지와 페이지 프레임의 매핑 정보를 저장하는 데이터 구조. 프로세스마다 페이지 테이블은 따로 존재
  + 주소 변환 : OS는 MMU(Memory Management Unit)라는 하드웨어 장치를 통해 가상 주소를 물리적 주소로 변환
  + 프로세스가 특정 가상 주소에 접근하면 MMU는 페이지 테이블을 참조하여 해당 가상 주소에 매핑된 물리적 주소를 찾고, 접근함
  + 페이지 폴트(Page Fault) : 프로세스가 물리적 메모리에 존재하지 않는 페이지를 접근하려고 할때 발생하는 것
  + 운영체제는 페이지 폴트가 발생하면 해당 페이지를 디스크의 스왑(swap) 영역에서 물리적 메모리에 로드
  + 스왑(Swap) 영역 : OS는 물리적 메모리가 부족할때, 사용하지 않는 페이지를 디스크의 스왑영역에 임시로 저장하고 필요할때 다시 물리적 메모리로 호출
- 각 프로세스에게 독립된 메모리 공간을 제공하고 메모리 보호와 자원 관리를 최적화
- 페이지 폴트가 자주 발생하면 디스크I/O가 증가하게 되어 성능 저하 발생 (스래싱, Thrashing). 복잡한 메모리 관리

> 가상 메모리 관련한 용어 정리
- 페이징 : 가상 주소 공간과 물리적 메모리 공간을 고정된 크기의 블록으로 나누어 관리하는 방법
- 페이지 : 가상 메모리에서 고정된 크기의 블록 단위
- 프레임 : 물리적 메모리의 블록 단위. 페이지와 동일한 크기로 나누어지며 운영체제는 각 페이지를 페이지 프레임에 매핑
 - 페이지 테이블 : 페이지와 페이지 프레임의 매핑 정보를 저장하는 데이터 구조. 프로세스마다 페이지 테이블은 따로 존재
 - 주소 변환 : OS는 MMU(Memory Management Unit)라는 하드웨어 장치를 통해 가상 주소를 물리적 주소로 변환
 - 프로세스가 특정 가상 주소에 접근하면 MMU는 페이지 테이블을 참조하여 해당 가상 주소에 매핑된 물리적 주소를 찾고, 접근함
 - 페이지 폴트(Page Fault) : 프로세스가 물리적 메모리에 존재하지 않는 페이지를 접근하려고 할때 발생하는 것
 - 운영체제는 페이지 폴트가 발생하면 해당 페이지를 디스크의 스왑(swap) 영역에서 물리적 메모리에 로드
 - 스왑(Swap) 영역 : OS는 물리적 메모리가 부족할때, 사용하지 않는 페이지를 디스크의 스왑영역에 임시로 저장하고 필요할때 다시 물리적 메모리로 호출

## 페이지 테이블(Page Table)이란 무엇이며, 그 역할을 설명해보세요.
- 가상 메모리에서 가상 주소를 실제 주소의 매핑 테이블 
- 페이지 테이블은 프로세스마다 하나씩 존재하며, 메인 메모리에 저장
> 페이지 테이블 역할
- 가상 주소와 물리 주소 매핑
  + 가상 주소는 페이지 번호와 오프셋으로 분리되어, 페이지 테이블을 통해 페이지 번호가 물리적 메모리 프레임 번호로 변환되고, 최종적으로 물리 주소 생성
- 페이지 폴트 확인
  + 페이지 테이블에는 각 페이지가 유효한지에 대한 상태를 나타내는 비트가 포함되어 있음
- 접근 권한 관리
  + 페이지 테이블은 각 페이지의 접근 권한(읽기, 쓰기, 실행 가능 여부)을 저장
  + 프로세스가 페이지에 접근할때 확인하고, 권한이 없을 경우 접근 차단
- 프로세스간 메모리 격리
  + 각 프로세스에 독립적인 페이지 테이블이 할당되어 각 프로세스는 다른 프로세스의 메모리 공간을 참조하지 못함

## 페이지 교체 알고리즘(FIFO, LRU, Optimal 등)의 차이점을 설명하세요.
> FIFO(First-In-First-Out) 알고리즘
- 가장 먼저 메모리에 적재된 페이지를 먼저 내보내는 방식
- 페이지가 물리 메모리에 들어온 순서대로 관리하고, 새로운 페이지가 필요할 때 가장 오래된 페이지를 제거하고 새 페이지 추가
- 장점 : 구현이 간단, 시간 순서대로 페이지 관리 가능
- 단점 : 가장 오래된 페이지가 자주 사용되는 페이지일 경우 성능 저하 발생 가능

> LRU(Least Recently Used) 알고리즘
- 가장 오랫동안 사용하지 않은 페이지를 교체하는 방식
- 각 페이지의 마지막 사용 시간을 추척하여 가장 오래 사용되지않은 페이지를 내보내는 방식
- 장점 : 자주 사용되는 페이지는 유지될 가능성이 높아 효율적
- 단점 : 사용 시점 관리를 위한 오버헤드가 발생할수 있고, 오버헤드가 높음

> LFU(Least Frequently Used) 알고리즘
- 사용 빈도가 적은 페이지를 교체하는 방식
- 페이지별로 참조 횟수를 기록하여 가장 적게 참조된 페이지를 내보냄
- 장점 : 참조 횟수가 낮은 페이지를 효율적으로 제거하여 메모리 사용률 증가
- 단점 : 최근 사용이 적어도 과거에 참조 횟수가 많은 경우에는 계속 남아있을 수가 있어서 성능 저하 

> OPT (Optimal replacement, 최적 교체)
- 앞으로 가장 오랫동안 사용되지 않을 거같은 페이지를 교체하는 방식
- 앞으로의 메모리 접근 순서를 알고 있다 가정하고, 다음 참조 시점이 가장 늦은 페이지를 제거.
- 페이지 폴트 수를 최소화 할 수 있는 이론적인 알고리즘
- 장점 : 모든 교체 알고리즘 중 가장 적은 페이지 폴트 발생
- 단점 : 실제 구현이 불가능. 비교용으로 사용 


## 페이지 폴트(Page Fault)란 무엇이며, 어떻게 처리되나요?
- 프로세스가 참조하려는 페이지에 접근하려고 했을 때 해당 페이지가 실제 물리 메모리에 없는 경우 뜨는 인터럽트
- 페이지 폴트가 발생하면 해당 페이지를 가상 메모리에서 찾아야하는데 이를 요구 페이징이라고 함
  
> 처리 과정 (요구 페이징)
- CPU는 물리 메모리를 확인 -> 페이지가 없으면 TRAP을 발생하여 운영체제에 알림
- 운영체제는 CPU의 동작을 잠시 멈춤
- 운영체제는 페이지 테이블을 확인하여 가상메모리에 페이지가 존재하는지 확인. 없으면 프로세스 중단
- 페이지 폴트이면 현재 물리 메모리에 비어있는 프레임이 있는지 찾음
- 비어있는 프레임에 해당 페이지 로드하고, 페이지 테이블을 업데이트
- 만약 비어있는 프레임이 없다면, 다른 프레임을 선택해서 이를 가상 메모리에 저장한후 필요한 페이지를 물리 메모리에 로드해야함 -> 페이지 교체 알고리즘 사용 (FIFO, LRU, Optimal)
- 중단된 CPU 다시 시작 

## TLB(Translation Lookaside Buffer)의 역할은 무엇인가요?
- 가상 메모리 주소를 물리적인 주소로 변환하는 속도를 높이기 위해 사용되는 일종의 주소 변환 캐시 

> 역할
- 페이지 테이블 조회 속도 향상
  + 가상 주소를 물리 주소로 변환하는 과정은 두번의 물리 주소를 참조( 해당 페이지 테이블 항목 참조, 요구된 데이터 접근을 위한 참조) 하면서 2배의 메모리 접근 시간을 갖게되는데, TLB는 최근에 참조한 페이지 번호와 해당 물리 주소의 매핑 정보를 저장하여, 동일한 페이지가 다시 참조될때 빠르게 변환을 처리할 수 있게 도와줌
- 메모리 접근 패턴이 일정할 경우 TLB가 큰 성능 이점을 제공 


## 스와핑(Swapping)이란 무엇이며, 언제 발생하나요?
- 물리 메모리가 부족할 때, 현재 필요하지 않는 프로세스를 메모리에서 내보내서 디스크의 swap 공간으로 이동시키는 메모리 관리 기법
> 발생 시점
- 물리 메모리 부족 : 실행중인 프로세스가 많아 메모리 공간이 부족한 경우
- 페이지 폴트 처리 시 : 페이지 폴트가 발생했으나, 메모리 공간이 부족하여 새로운 페이지를 로드할 수 없는 경우


## 세그멘테이션(Segmentation)과 페이징(Paging)의 차이점은 무엇인가요?
> 세그멘테이션 (Segmentation)
- 메모리를 효율적으로 관리하기 위해 프로그램을 논리적 구성 요소 단위인 세그먼트(segment)로 나누는 메모리 기법 관리
- 세그먼트(segment) : 프로세스를 서로 크기가 다른 논리적인 블록 단위로 분할
- 세그먼트 테이블 : 세크먼트 번호, 세그먼트의 시작 주소(베이스)와 세그먼트 크기(리미트)를 포함하여 CPU가 논리 주소를 물리 주소로 변환할 때 사용
- 페이징과 유사하지만, 다른점은 세그먼트의 크기가 일정하지 않으므로 limit 정보가 주어짐. CPU에서 해당 세그먼트의 크기를 넘어서는 주소가 들어오면 인터럽트 발생 -> 프로세스 강제 종료

> 차이점
- 분할 기준 차이
  + 세그멘테이션 : 논리적 구조에 따라 크기가 다른 세그먼트로 분할
  + 페이징 : 프로세스 상관없이 메모리를 고정된 크기의 블록으로 분할. 모든 페이지의 크기 동일
- 단편화 문제
  + 세그멘테이션 : 각 세그먼트가 다른 크기를 가질 수 있으므로 외부 단편화 발생 가능성
  + 페이징 : 고정된 크기의 페이지로 분할하므로 외부 단편화는 없지만, 내부 단편화 발생 가능성
- 페이징의 주소 변환과 메모리 관리가 간단하고, CPU에서 페이징을 위한 MMU를 지원해주므로 실무에서는 주로 페이징이 더 많이 사용 

## 메모리 단편화(Fragmentation)란 무엇이며, 내부 단편화와 외부 단편화의 차이점은 무엇인가요?
> 메모리 단편화
- 메모리의 공간이 작은 조각으로 나뉘어져 사용가능한 메모리가 충분히 존재하지만 사용이 불가능한 상태 

> 외부 단편화(External Fragmentation)
- 메모리가 할당 및 해제 작업의 반복으로, 비연속적인 작은 조각들로 나뉘어 있어서 원하는 크기의 메모리를 할당할 수 없는 상황
- 메모리 압축 또는 페이징과 같은 고정 크기 분할를 사용하여 외부 단편화를 줄일수 있음
  
> 내부 단편화(Internal Fragmentation)
- 메모리 블록이 고정된 크기로 할당되었을때, 실제로 사용한 메모리보다 할당된 블록이 커서 남는 공간이 발생하는 상황
- 블록 크기의 상황에 맞게 크기를 조정하거나, 세그멘테이션과 같은 가변 크기 할당 기법을 사용 

## 페이지 크기(Page Size)가 메모리 관리에 미치는 영향은 무엇인가요?
- Page Size : OS가 메모리를 관리할 때 사용하는 고정된 메모리 블록의 크기
- Paging : 페이지와 프레임을 매핑하여 메모리를 관리하는 방식
- Page Fault : 프로세스가 접근할 페이지가 메모리에 없는 경우
  
> 단편화
- 작은 페이지 크기 : 내부 단편화는 줄어들지만, 페이지 테이블의 항목 수가 많아져 더 많은 메모리를 사용할 수 있음
- 큰 페이지 크기 : 내부 단편화가 발생할 가능성이 높음 
> 페이지 교체 및 페이지 폴트 비용
- 작은 페이지 크기 : 페이지 폴트 발생시 불필요한 데이터를 적게 불러오므로 효율적 
- 큰 페이지 크기 : 한번의 페이지 폴트 시 더 많은 데이터를 불러올 수 있지만, 불필요한 데이터도 불러올 가능성이 있으므로 메모리 공간이 낭비될 수 있음 
> TLB 효율성
- 작은 페이지 크기 : TLB에 많은 페이지 매핑 정보를 저장할 수 있어 세부적으로 관리 가능 
- 큰 페이지 크기 : 한번의 매핑으로 많은 메모리 관리가 가능하지만 메모리가 낭비될 가능성도 있음 
> 디스크 I/O 및 캐시 효율
- 작은 페이지 크기 : 필요한 데이터만 불러올수 있을 가능성이 높지만, 디스크 I/O가 자주 발생하여 성능이 저하될수 있음 
- 큰 페이지 크기 : 데이터를 불러오는 I/O 횟수가 줄어서 전체적인 I/O 성능은 향상될수 있지만, 메모리 공간이 낭비될수 있음

## SSD와 HDD의 차이점은 무엇인가요?
> SSD (Solid State Drive)
- 작동 원리 : 반도체 메모리를 기반으로 데이터 저장. NAND 플래시 메모리 셀을 통해 전자적으로 데이터를 읽고 쓰기 때문에 물리적 손상에 강함 
- 성능 : 기계적 지연시간이 없으므로 데이터 전송 속도와 접근 속도가 매우 빠름 
- 내구성 : 물리적 충격에는 강하지만, 쓰기 수명에 한계 존재. 
- 사용 사례 : 빠른 속도가 중요한 경우 적합. 게임 로딩, DB 서버 등 

> HDD (Hard Disk Drive)
- 작동 원리 : 자기 디스크 위에 데이터를 저장. 디스크가 회전하며 읽고 쓰기 헤드가 디스크 위를 이동하여 데이터를 읽고 씀. 물리적으로 디스크와 헤드가 움직이므로 기계적 지연 시간이 발생
- 성능 : SSD에 비해 상대적으로 데이터 전송속도와 접근 속도는 느림
- 내구성 : 기계적 부품이 움직이므로 물리적 충격에 약하며, 시간이 지나면 기계적 마모가 발생 
- 사용 사례 : 대용량 데이터 저장이 중요한 경우 적합. 데이터 백업, CCTV 영상 저장 등 

# 캐시
## 캐시(Caching)는 운영 체제에서 어떤 역할을 하며, 성능 향상에 어떻게 기여하는가요?
- Caching : 자주 사용하는 데이터나 명령어를 빠른 접근 속도를 가진 메모리에 임시로 저장하여 필요할때 빠르게 사용할 수 있게 하는 기술

> 역할
- 데이터 접근 속도를 높여 CPU와 메모리 간의 속도 차이를 줄이기 위해 사용

> 기여 
- CPU 캐시를 통한 데이터 접근 시간 단축
  + CPU에 있는 L1, L2, L3 캐시에 자주 사용하는 명령어와 데이터 저장
  + CPU가 캐시에 있는 데이터에 접근할 수 있으므로 메인 메모리까지의 데이터 요청시간을 줄여줌
- 디스크 캐시로 디스크 I/O 성능 최적화
  + 디스크에서 자주 읽히는 파일을 메인 메모리의 디스크 캐시에 저장
  + 디스크 I/O를 줄여 접근 시간을 단축하고 응답 속도 향상 
- 페이지 캐시를 통해 가상 메모리 성능 향상
  + 자주 사용되는 페이지를 메모리에 캐시하여 페이지 폴트 감소
  + 메모리 접근 성능 향상 
  
## 캐시 히트(Cache Hit)와 캐시 미스(Cache Miss)란 무엇이며, 캐시 히트율(Cache Hit Rate)은 어떻게 측정되는가요?
> 캐시 히트(Cache Hit)
- 요청한 데이터가 캐시에 존재해서 바로 데이터를 가져올 수 있는 경우
> 캐시 미스(Cache Miss)
- 요청한 데이터가 캐시에 없어서 메모리나 디스크에서 데이터를 가져와야 하는 경우 
> 캐시 히트율(Cache Hit Rate)
- 전체 캐시 요청 중에서 캐시 히트가 발생한 비율
- 캐시 히트율이 높다는 것은 대부분의 요청이 캐시에서 처리된다는 뜻 

## 캐시의 공간적 지역성과 시간적 지역성에 대해서 설명해주세요
> 공간적 지역성
- 특정 데이터와 인접한 주소의 데이터가 참조될 가능성이 높다는 원칙
- 캐시가 데이터를 블록 단위로 로드하여 데이터 접근 패턴을 최적화 -> 캐시 미스를 줄이고 접근 시간 단축에 기여 가능
- 예) 배열과 같은 자료구조에 순차적으로 접근할 가능성이 크다는 원칙 -> 캐시는 배열의 인접 요소를 미리 로드
> 시간적 지역성
- 특정 데이터가 한번 참조된 경우에 가까운 시점에 또 한번 참조될 가능성이 높다는 원칙
- 캐시 히트율을 높여 성능 향상시킬수 있음
- 예) CPU가 특정 변수를 반복해서 참조하는 경우, 해당 변수를 캐시에 저장 

## 버퍼 캐시(Buffer Cache)와 페이지 캐시(Page Cache)의 차이점은 무엇이며, 파일 시스템에서 각각의 역할은 무엇인가요?
> 버퍼 캐시(Buffer Cache)
- 디스크 블록 단위의 데이터를 캐싱하는 메모리 공간
- 블록 : 디스크의 저장 공간을 효율적으로 관리하기 위해 나눈 고정 크기의 데이터 단위 (물리적 저장 장치의 단위) 
- 블록 단위로 데이터를 버퍼 캐시에 저장해 두면, 동일한 블록에 대한 요청시 디스크 접근 없이 버퍼 캐시에서 데이터 접근 가능
- 디스크와 관련된 블록 데이터의 I/O 요청을 효율적으로 관리 
- 파일 시스템에서의 역할 : 디스크 블록 단위로 데이터를 캐싱하여 디스크 I/O 성능을 높이고, 디스크로의 직접 접근을 최소화 

> 페이지 캐시(Page Cache)
- 처리한 데이터를 메인 메모리 영역(RAM)에 저장해서 가지고 있다가, 이 데이터에 대한 접근이 발생하면 디스크에서 IO 처리를 하지 않고 메인 메모리 영역의 데이터를 반환하여 처리할 수 있도록 하는 메모리 공간
- 파일 : 데이터를 접근하고 관리하는 논리적인 단위 
- 파일 시스템의 데이터 접근을 빠르게 지원 
- 파일 시스템에서의 역할 : 자주 읽고 쓰는 파일 데이터를 페이지 캐시에 저장하여 디스크에 접근하지 않고 빠르게 데이터를 읽고 쓸수 있도록 하는 역할 

# 파일 시스템
## 파일 시스템이란 무엇인가요?
- 운영체제에서 하드 디스크, SSD와 같은 저장 장치에 있는 파일을 구성하고 관리하는데 사용하는 구조
- 저장 장치에 데이터를 저장, 접근 및 구성하는 방법을 정의

## 파일 시스템에서 inode의 역할은 무엇인가요?
- 파일 시스템에서 파일이나 디렉터리와 같은 파일 시스템 개체를 표현하는 자료구조 (EX. Unix)
- 파일에 대한 메타데이터 저장
  + 파일 크기, 파일의 생성 및 수정 시간, 소유자 및 권한 정보, 파일 유형, 파일을 참조하는 하드 링크의 개수
- 파일의 데이터 위치 관리
  + 파일의 데이터가 저장된 실제 디스크 블록들의 위치 정보를 가지고 있음 ( 인덱스 역할 )
- 파일 고유 식별자
  + 모든 파일은 고유한 inode 번호를 가짐 

## 파일 디스크립터(File Descriptor)는 무엇인가요?
- 시스템으로부터 할당 받은 파일을 대표하는 음수가 아닌 정수값 
- 프로세스마다 고유한 식별자 역할
> 리눅스에서의 동작 원리 
- 파일 열기 : 파일을 열면 리눅스 커널은 해당 파일을 열고 새로운 파일 디스크립터 번호를 할당
- 파일 입출력 :프로세스가 파일에 데이터를 읽기 또는 쓰기 작업을 수행할때 리늑스는 파일 디스크립터를 사용하여 해당 파일의 위치와 상태 확인
- 파일 닫기 : 파일 사용이 끝나면 close() 호출을 통해 파일 디스크립터 해제. 파일 디스크립터 테이블에서 해당 항목이 삭제되고 다른 파일을 열때 해당 번호 재사용 가능
- lsof : 현재 시스템에서 열려 있는 모든 파일 표시
- /proc : 특정 프로세스의 파일 디스크립터 확인할때 사용 ( ls -l /proc/<PID>/fd )

## 파일 시스템에서 링크(link)란 무엇이며, 하드 링크와 심볼릭 링크의 차이점에 대해서 설명해주세요
> 링크(link)
- 파일 시스템 상에 존재하는 파일/디렉토리에 대한 바로가기를 만들 때 사용되는 파일 ( 기존 파일에 새로운 파일명을 붙이는 것 )

> 하드 링크
- 원본 파일과 동일한 inode를 직접적으로 가리켜서 원본 파일이 사라지더라도 데이터만 살아있다면 원본 파일에 접근 가능 ( 데이터를 안전하게 관리하고자 할때 주로 사용 )
- 원본 파일에 대한 하드 링크를 만들면 링크 수가 증가 -> 원본이 삭제가 되더라도 링크수가 남아있다면 데이터는 유지
- 동일한 파일 시스템 내에서만 하드링크 생성 가능
  
> 심볼릭 링크
- 또다른 inode를 생성하는데 복사 생성된 inode는 포인터를 가리키고, 포인터는 다시 원본 파일을 가리킴
- 하드 링크의 단점을 보완하기 위해 탄생
- 파일이나 디렉토리를 가리킬 텍스트 포인터를 가지므로 참조 가능
- 대상 파일의 inode가 아닌 대상 파일의 데이터 경로를 참조
- 원본 파일이 사라지면 해당 데이터에 접근할 수 없음

## 저널링 파일 시스템(Journaling File System)이란 무엇인가요?
- 메인 파일 시스템에 변경사항을 반영하기전에, 저널(파일 시스템의 지정된 영역 안의 원형 로그)안에 생성되는 변경사항을 추적하는 파일 시스템.
- 저널링 파일 시스템은 파일 시스템의 데이터 손상을 방지하고, 시스템 장애나 예기치 않은 종료 이후 빠르게 복구할수 있도록 설계

> 저널링 파일 시스템은 운영체제에서 필수인지?
- 필수는 아니지만, 데이터 안정성과 복구를 위해 선택적으로 사용
- 임시 파일을 저정하는 환경에서는 저널링이 필요없을수도있음
- 저널링은 데이터를 쓸때 저널에 기록하는 추가 작업이 필요해서 일부 경우에는 성능이 저하될수도 있어서 속도가 중요한 환경에서는 저널링 파일시스템을 사용하지 않는 경우도 있음

## 디스크 스케줄링이 필요한 이유와 다양한 디스크 스케줄링 알고리즘(FCFS, SSTF, SCAN 등)에 대해서 설명해주세요.
> 디스크 스케줄링이 필요한 이유
- 디스크 헤드의 물리적 이동은 시간이 많이 소요되므로, 요청을 최적의 순서로 처리하여 헤드 이동거리를 줄이기 위해
- 여러 프로세스가 디스크 접근시 대기 시간을 줄이고 작업의 처리율을 높이기 위해

> FCFS(First Come First Served)
- 큐에 가장 먼저 요청이 온 순서대로 디스크 접근을 처리하는 방식
- 장점 : 구현이 간단하고 공평하게 요청 처리 가능
- 단점 : 헤드가 무작위로 이동하게 되므로 전체 이동 거리가 길어지면서 디스크 접근 시간이 비효율적일 수 있음 
  
> SSTF (Shortest Seek Time First)
- 현재 헤드에서 가장 가까운 트랙의 요청을 먼저 처리하는 방식
- 장점 : 헤드 이동 거리를 최소화하여 접근 시간 줄일 수 있음
- 단점 : 헤드에서 거리가 먼 요청들은 기아 현상이 발생할 수 있고, 응답시간 편차가 큼
 
> SCAN
- 헤드의 진행방향에 있는 요청을 처리하고, 다시 반대방향으로 틀어 반대방향에 있는 요청을 처리하는 방식
- 장점 : SSTF의 기아 가능성을 제거할 수 있고, 응답시간의 편차 줄일수 있음
- 단점 : 헤드가 끝까지 이동한 후 돌아와야하므로, 끝에 있는 요청이 처리될 때까지 지연될수 있음

> C-SCAN (Circular SCAN)
- SCAN 방식과 유사하지만, 헤드가 한쪽 끝까지 도달하면 처음으로 돌아와 다시 시작 (한쪽방향으로만 스캔)해서 요청을 처리하는 방식
- 장점 : 모든 요청이 고르게 처리되며, 요청 지연시간이 일정
- 단점 : 처리할 요청이 없어도 헤드가 끝까지 이동하므로 비효율적 
  
> LOOK / C-LOOK
- SCAN과 C-SCAN을 보완하기 위한 처리 방식으로 , 요청이 진행방향에 더이상 없다면 끝까지 가지않고 반대방향으로 가거나(SCAN), 다시 시작 단계로 돌아와 같은 방향으로 진행(C-SCAN)
- 장점 : 불필요한 헤드 이동시간 제거
- 단점 : 끝단까지 가야하는지 여부를 판단하는데 있어서 오버헤드 발생

## 운영체제에서 마운트란 무엇이며, 운영체제에서 어떻게 동작되는가
- 리눅스에서는 하드디스크의 파티션, USB 등을 사용하려면 특정 위치에 연결을 해줘야하는데, 이런 물리적인 장치를 특정한 위치(디렉토리)에 연결시켜 주는 과정

> 동작 원리 (ex. usb 드라이브)
1. USB 드라이브 인식
- USB를 연결하면 운영체제는 해당 장치를 인식하고, 파일 시스템 타입을 확인 (USB는 FAT32나 exFAT 파일 시스템으로 포맷 되어있음)
- 운영체제는 USB의 파일 시스템 타입을 읽고 해당 파일 시스템을 처리할 수 있는 드라이버를 호출
2. 마운트 포인트 설정
- 파일 시스템 타입을 인식한 후, 운영체제는 USB 드라이브를 시스템 디렉토리 트리의 특정 위치에 연결
- 윈도우 -> 새 드라이브(E:, F:)로 마운트 / 리눅스 -> /media/usb 같은 폴터에 마운트
- 해당 폴더가 마운트 포인트  -> 이 폴더를 통해 USB에 접근
3. 마운트 테이블에 기록
- USB가 마운트되면 운영체제는 마운트 정보를 마운트 테이블에 기록 (디렉토리 경로, 파일 시스템 종류 등 )
- 마운트 테이블은 운영체제가 장치에 접근할 때 참조하는 데이터
4. 파일 접근, 가상 파일 시스템(Virtual File System, VFS)
- 운영체제는 사용자가 /media/usb/test.txt 경로로 파일에 접근하면 가상 파일 시스템 계층을 통해 마운트 테이블에서 USB가 연결된 위치를 확인하고 , 파일 시스템 드라이버를 호출하여 USB의 파일에 접근
5. 마운트 해제 (Unmount)
- 운영체제는 마운트 해제 작업을 통해 마운트 테이블에서 USB 정보를 제거하고, 파일 시스템 캐시 내용을 USB에 기록하여 데이터 손상 방지. 

# 병렬 시스템

## 동기와 비동기 그리고 병렬성에 대한 차이점에 대해서 설명해주세요.
> 동기
- 작업을 순서대로 수행하고, 현재 작업이 완료되어야 다음 작업을 진행할 수 있는 방식 
> 비동기
- 작업을 요청한 후, 해당 작업이 완료되기를 기다리지 않고 즉시 다음 작업을 진행하는 방식 
> 병렬성
- 여러 작업을 물리적으로 동시에 수행하는 방식. 멀티코어 방식에서 각 작업을 동시에 처리하여 전체 처리속도 증가

## CPU 개수가 증가할수록 시스템의 성능이 좋아지는 이유에 대해서 설명해주세요.
- 멀티 스레드인 프로그램에서 각 스레드를 별도의 코어에서 병렬로 나누어 처리하면, 단일 코어에서 순차적으로 실행하는 것보다 특정 코어에 작업이 몰리는 현상이 줄고, 작업이 대기 상태로 머무는 시간이 짧아지면서 성능이 높아짐.

## 멀티코어 CPU 환경에서 자바의 병렬 처리 성능을 최적화하기 위해 코어 수와 스레드 수의 균형을 어떻게 맞춰야 하는가요?

## CPU 바운드(CPU-bound) 작업과 I/O 바운드(I/O-bound) 작업의 특성에 따라 병렬 처리 전략을 어떻게 달리 설정해야 하는가?

## JVM에서 Garbage Collection이 병렬 처리 성능에 미치는 영향은 무엇이며, 이를 최적화하기 위한 JVM 튜닝 방법은?


















